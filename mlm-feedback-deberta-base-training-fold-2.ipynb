{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, lr_scheduler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport time\nimport os\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:01.867097Z","iopub.execute_input":"2022-08-22T07:34:01.867749Z","iopub.status.idle":"2022-08-22T07:34:04.327317Z","shell.execute_reply.started":"2022-08-22T07:34:01.867658Z","shell.execute_reply":"2022-08-22T07:34:04.326334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:04.328874Z","iopub.execute_input":"2022-08-22T07:34:04.329522Z","iopub.status.idle":"2022-08-22T07:34:04.336136Z","shell.execute_reply.started":"2022-08-22T07:34:04.329483Z","shell.execute_reply":"2022-08-22T07:34:04.335016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    'batch_size': 2,\n    'epochs': 5,\n    'lr': 8e-6,\n    'n_accumulate': 8,\n#     'pretrained_model_path': '../input/deberta-base/mlm_base/mlm_base/Deberta_large-v3-itpt-e3',\n#     'pretrained_config_path': '../input/deberta-base/mlm_base/mlm_base/model_tokenizer',\n#     'model_name': '../input/deberta-v3-large',\n    'model_name': 'microsoft/deberta-v3-large',\n    'weight_decay': 1e-4,\n    'seed': 2022,\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'patience': 10,\n    'num_cycles':0.5,\n    'scheduler':'cosine', # ['linear', 'cosine']\n    'batch_scheduler':True,\n    'num_warmup_steps':0,\n    'folds':[3],\n    'saved_model_path': \"saved_model_state_deberta.pt\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:04.337754Z","iopub.execute_input":"2022-08-22T07:34:04.338458Z","iopub.status.idle":"2022-08-22T07:34:04.387339Z","shell.execute_reply.started":"2022-08-22T07:34:04.338421Z","shell.execute_reply":"2022-08-22T07:34:04.386242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/feedback-stratified-folds-disctype-eff/feedback_train_folds.csv')\ntest_data = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\n\ndef fetch_essay_texts(df, train=True):\n    if train:\n        base_path = '../input/feedback-prize-effectiveness/train/'\n    else:\n        base_path = '../input/feedback-prize-effectiveness/test/'\n        \n    essay_texts = {}\n    for filename in os.listdir(base_path):\n        with open(base_path + filename) as f:\n            text = f.readlines()\n            full_text = ' '.join([x for x in text])\n            essay_text = ' '.join([x for x in full_text.split()])\n        essay_texts[filename[:-4]] = essay_text\n    df['essay_text'] = [essay_texts[essay_id] for essay_id in df['essay_id'].values]   \n    return df\n    \ndata = fetch_essay_texts(data)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:04.390834Z","iopub.execute_input":"2022-08-22T07:34:04.391740Z","iopub.status.idle":"2022-08-22T07:34:07.113017Z","shell.execute_reply.started":"2022-08-22T07:34:04.391703Z","shell.execute_reply":"2022-08-22T07:34:07.112013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {'Effective': 0, 'Adequate': 1, 'Ineffective': 2}\ndata['discourse_effectiveness'].replace(label_dict, inplace=True)\ndata.drop(['discourse_id', 'essay_id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:07.114857Z","iopub.execute_input":"2022-08-22T07:34:07.115610Z","iopub.status.idle":"2022-08-22T07:34:07.148071Z","shell.execute_reply.started":"2022-08-22T07:34:07.115571Z","shell.execute_reply":"2022-08-22T07:34:07.147209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=CONFIG['seed']):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:07.149617Z","iopub.execute_input":"2022-08-22T07:34:07.149989Z","iopub.status.idle":"2022-08-22T07:34:07.158245Z","shell.execute_reply.started":"2022-08-22T07:34:07.149953Z","shell.execute_reply":"2022-08-22T07:34:07.157128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ModelEmaV2(nn.Module):\n#     def __init__(self, model, decay=0.9995, device=None):\n#         super(ModelEmaV2, self).__init__()\n#         # make a copy of the model for accumulating moving average of weights\n#         self.module = deepcopy(model)\n#         self.module.eval()\n#         self.decay = decay\n#         self.device = device  # perform ema on different device from model if set\n#         if self.device is not None:\n#             self.module.to(device=device)\n\n#     def _update(self, model, update_fn):\n#         with torch.no_grad():\n#             for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n#                 if self.device is not None:\n#                     model_v = model_v.to(device=self.device)\n#                 ema_v.copy_(update_fn(ema_v, model_v))\n\n#     def update(self, model):\n#         self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n#     def set(self, model):\n#         self._update(model, update_fn=lambda e, m: m)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:07.160050Z","iopub.execute_input":"2022-08-22T07:34:07.160919Z","iopub.status.idle":"2022-08-22T07:34:07.168508Z","shell.execute_reply.started":"2022-08-22T07:34:07.160882Z","shell.execute_reply":"2022-08-22T07:34:07.167401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, tokenizer, essay_text, discourse_type, discourse_text, effectiveness):\n        self.essay_text = essay_text\n        self.discourse_type = discourse_type\n        self.discourse_text = discourse_text\n        self.target = effectiveness\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.discourse_text)\n    \n    def __getitem__(self, idx):\n        input_discourse = self.discourse_type[idx] + ' ' + self.tokenizer.sep_token + ' ' + self.discourse_text[idx]\n        tokenized_discourse = self.tokenizer.encode_plus(\n                            input_discourse,\n                            return_token_type_ids=False,\n                            return_attention_mask=True,\n                            max_length=512,\n                            truncation=True,\n                            padding='max_length',\n                            add_special_tokens=True,\n                            return_tensors='pt'\n                        )\n        tokenized_essay = self.tokenizer.encode_plus(\n                            self.essay_text[idx],\n                            return_token_type_ids=False,\n                            return_attention_mask=True,\n                            max_length=512,\n                            truncation=True,\n                            padding='max_length',\n                            add_special_tokens=True,\n                            return_tensors='pt',\n                        )\n        return {\n            'discourse_input_ids': tokenized_discourse['input_ids'].flatten(),\n            'discourse_attention_mask': tokenized_discourse['attention_mask'].flatten(),\n            'essay_input_ids': tokenized_essay['input_ids'].flatten(),\n            'essay_attention_mask': tokenized_essay['attention_mask'].flatten(),\n            'target': self.target[idx]\n        }","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:07.170369Z","iopub.execute_input":"2022-08-22T07:34:07.171221Z","iopub.status.idle":"2022-08-22T07:34:07.184107Z","shell.execute_reply.started":"2022-08-22T07:34:07.171185Z","shell.execute_reply":"2022-08-22T07:34:07.183122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n# from transformers import GPT2Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:07.185683Z","iopub.execute_input":"2022-08-22T07:34:07.186128Z","iopub.status.idle":"2022-08-22T07:34:10.167083Z","shell.execute_reply.started":"2022-08-22T07:34:07.186092Z","shell.execute_reply":"2022-08-22T07:34:10.166052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPoolingLayer(nn.Module):\n    def __init__(self):\n        super(MeanPoolingLayer, self).__init__()\n    \n    def forward(self, last_hidden_state, attention_mask):\n        expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        mask_sum = expanded_mask.sum(1)\n        mask_sum = torch.clamp(mask_sum, min=1e-9)\n        masked_hidden_state = torch.sum(last_hidden_state * expanded_mask, 1)\n        return masked_hidden_state / mask_sum","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:10.168789Z","iopub.execute_input":"2022-08-22T07:34:10.169189Z","iopub.status.idle":"2022-08-22T07:34:10.176605Z","shell.execute_reply.started":"2022-08-22T07:34:10.169149Z","shell.execute_reply":"2022-08-22T07:34:10.175255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:10.178476Z","iopub.execute_input":"2022-08-22T07:34:10.178832Z","iopub.status.idle":"2022-08-22T07:34:10.195562Z","shell.execute_reply.started":"2022-08-22T07:34:10.178795Z","shell.execute_reply":"2022-08-22T07:34:10.194294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiscourseEffectivenessModel(nn.Module):\n    def __init__(self, num_classes=3, config_path=None):\n        super(DiscourseEffectivenessModel, self).__init__()\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(CONFIG['model_name'], output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        self.num_classes = num_classes\n#         self.pretrained_layer = AutoModel.from_pretrained(CONFIG['pretrained_model_path'])\n        self.pretrained_layer = AutoModel.from_pretrained(CONFIG['model_name'])\n        \n        self.pooler = MeanPoolingLayer()\n#         self.pooler = WeightedLayerPooling()\n        self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2, \n                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n                              bidirectional=True)\n        self.dropout = nn.Dropout(p=0.3)\n        self.fc = nn.Linear(3*self.pretrained_layer.config.hidden_size, num_classes)\n    \n    def forward(self, discourse_input_ids, discourse_attention_mask, essay_input_ids, essay_attention_mask):\n        discourse_out = self.pretrained_layer(input_ids=discourse_input_ids, attention_mask=discourse_attention_mask)\n        discourse_emb = self.pooler(discourse_out.last_hidden_state, discourse_attention_mask)\n        essay_out = self.pretrained_layer(input_ids=essay_input_ids, attention_mask=essay_attention_mask)\n        essay_emb = self.pooler(essay_out.last_hidden_state, essay_attention_mask)\n        concat_emb = torch.cat([discourse_emb, essay_emb, torch.abs(essay_emb - discourse_emb)], dim=-1)\n        x = self.dropout(concat_emb)\n        x = self.fc(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:10.197332Z","iopub.execute_input":"2022-08-22T07:34:10.198275Z","iopub.status.idle":"2022-08-22T07:34:10.212025Z","shell.execute_reply.started":"2022-08-22T07:34:10.198238Z","shell.execute_reply":"2022-08-22T07:34:10.211091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DiscourseEffectivenessModel().to(CONFIG['device'])\n# model_ema = ModelEmaV2(model)\noptimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\nloss = nn.CrossEntropyLoss().to(CONFIG['device'])\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:10.216746Z","iopub.execute_input":"2022-08-22T07:34:10.217621Z","iopub.status.idle":"2022-08-22T07:34:19.569562Z","shell.execute_reply.started":"2022-08-22T07:34:10.217577Z","shell.execute_reply":"2022-08-22T07:34:19.568487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, dataloader, loss, optimizer, scheduler, model_ema=None):\n    model.train()\n    batch_losses = []\n    scaler = torch.cuda.amp.GradScaler()\n    for batch_num, batch in tqdm(enumerate(dataloader)):\n        discourse_input_ids = batch['discourse_input_ids'].to(CONFIG['device'], non_blocking=True)\n        discourse_attention_mask = batch['discourse_attention_mask'].to(CONFIG['device'], non_blocking=True)\n        essay_input_ids = batch['essay_input_ids'].to(CONFIG['device'], non_blocking=True)\n        essay_attention_mask = batch['essay_attention_mask'].to(CONFIG['device'], non_blocking=True)\n        targets = batch['target'].to(CONFIG['device'], non_blocking=True)\n        with torch.cuda.amp.autocast():\n            logits = model(discourse_input_ids, discourse_attention_mask, essay_input_ids, essay_attention_mask)\n            probs = nn.Softmax(dim=1)(logits)\n            \n        batch_loss = loss(logits, targets)\n        batch_loss = batch_loss / CONFIG['n_accumulate']\n        batch_losses.append(batch_loss.item())\n        scaler.scale(batch_loss).backward()\n        if (batch_num+1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            if CONFIG['batch_scheduler']:\n                scheduler.step()\n        \n        optimizer.step()\n#         if model_ema is not None:\n#             model_ema.update(model)\n    return np.mean(batch_losses)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:19.570902Z","iopub.execute_input":"2022-08-22T07:34:19.571304Z","iopub.status.idle":"2022-08-22T07:34:19.619445Z","shell.execute_reply.started":"2022-08-22T07:34:19.571264Z","shell.execute_reply":"2022-08-22T07:34:19.618153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_loop(model, loss, dataloader):\n    model.eval()\n    batch_losses = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            discourse_input_ids = batch['discourse_input_ids'].to(CONFIG['device'], non_blocking=True)\n            discourse_attention_mask = batch['discourse_attention_mask'].to(CONFIG['device'], non_blocking=True)\n            essay_input_ids = batch['essay_input_ids'].to(CONFIG['device'], non_blocking=True)\n            essay_attention_mask = batch['essay_attention_mask'].to(CONFIG['device'], non_blocking=True)\n            targets = batch['target'].to(CONFIG['device'], non_blocking=True)\n#             with torch.cuda.amp.autocast():\n            logits = model(discourse_input_ids, discourse_attention_mask, essay_input_ids, essay_attention_mask)\n            probs = nn.Softmax(dim=1)(logits)\n            batch_loss = loss(logits, targets)\n            batch_losses.append(batch_loss.item())\n    \n    return np.mean(batch_losses)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:19.621551Z","iopub.execute_input":"2022-08-22T07:34:19.621940Z","iopub.status.idle":"2022-08-22T07:34:19.634589Z","shell.execute_reply.started":"2022-08-22T07:34:19.621901Z","shell.execute_reply":"2022-08-22T07:34:19.633506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = defaultdict(list)\nbest_loss = np.inf\nprev_loss = np.inf\nearlystop_trigger = 0\nepochs = CONFIG['epochs']\nfor fold in CONFIG['folds']:\n    train_data = data[data['fold'] != fold].reset_index(drop=True)\n    val_data = data[data['fold'] == fold].reset_index(drop=True)\n    train_dataset = FeedbackDataset(tokenizer, \n                                train_data['essay_text'].values,\n                                train_data['discourse_type'].values, \n                                train_data['discourse_text'].values, \n                                train_data['discourse_effectiveness'].values)\n    val_dataset = FeedbackDataset(tokenizer, \n                              val_data['essay_text'].values,\n                              val_data['discourse_type'].values, \n                              val_data['discourse_text'].values, \n                              val_data['discourse_effectiveness'].values)\n    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, pin_memory=True)\n    \n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg['scheduler'] == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg['num_warmup_steps'], num_training_steps=num_train_steps\n            )\n        elif cfg['scheduler'] == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg['num_warmup_steps'], num_training_steps=num_train_steps, num_cycles=cfg['num_cycles']\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_data) / CONFIG['batch_size'] * CONFIG['epochs'])\n    scheduler = get_scheduler(CONFIG, optimizer, num_train_steps)\n    \n    print('Training Start ......')\n    for epoch in range(CONFIG['epochs']):\n        print(f'Epoch {epoch+1} of {epochs}')\n        train_loss = train_loop(model, train_dataloader, loss, optimizer, scheduler)\n        print(f'Training Loss: {train_loss}')\n        val_loss = validation_loop(model, loss, val_dataloader)\n        print(f'Val Loss: {val_loss}')\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        \n        if val_loss <= prev_loss:\n            earlystop_trigger = 0\n        else:\n            earlystop_trigger += 1\n            if earlystop_trigger >= CONFIG['patience']:\n                print(f'Early Stopping Triggered after {epoch+1} epochs. Aborting Training...')\n                break\n        if val_loss < best_loss:\n            print(f'New best val loss, saving model checkpoint at epoch {epoch+1}.')\n            torch.save(model.state_dict(), CONFIG['saved_model_path'])\n            best_loss = val_loss\n        prev_loss = val_loss","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:19.636327Z","iopub.execute_input":"2022-08-22T07:34:19.637015Z","iopub.status.idle":"2022-08-22T07:34:47.731473Z","shell.execute_reply.started":"2022-08-22T07:34:19.636975Z","shell.execute_reply":"2022-08-22T07:34:47.728805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Val Loss')\nplt.legend()\nplt.grid()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Diagnostic Curves')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T07:34:47.732615Z","iopub.status.idle":"2022-08-22T07:34:47.734448Z","shell.execute_reply.started":"2022-08-22T07:34:47.734181Z","shell.execute_reply":"2022-08-22T07:34:47.734207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}